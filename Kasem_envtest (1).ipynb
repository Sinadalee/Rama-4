{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junction_id = {'KLONGTEI': 'cluster_1892287670_272491964_272492178',\n",
    "      'RAMA4': 'cluster_272488163_282390730_66263210_66263222',\n",
    "      'NARANONG': 'cluster_272488164_272492179_3457051443_61907354',\n",
    "      'SUNLAKAKHON': 'gneJ83',\n",
    "      'KASEMRAT': 'cluster_272448137_272555800_272555808_7660045934_7710268409',\n",
    "      'ATTHAKAWI_RAMA4' : '270329335'}\n",
    "junction_name = list(junction_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "# from gym import error, spaces\n",
    "import gym\n",
    "import csv\n",
    "import os, sys\n",
    "sys.path.append(os.path.join('/home/ring/sumo-svn/', 'tools'))\n",
    "import traci\n",
    "import traci.constants as tc\n",
    "import numpy as np\n",
    "from sumolib import checkBinary\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "namelane_csv = pd.read_csv('namelane_KASEMRAT.csv')\n",
    "namelane_df = pd.DataFrame(namelane_csv, columns = ['name' , 'id'])\n",
    "NAME = namelane_df.set_index('name')\n",
    "ID = namelane_df.set_index('id')\n",
    "if NAME.loc['KASEMRAT_EB_0_0_XSXX','id'] == '459551209#3_0':\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "KASEMRAT_EB_0_0_XSXX                      459551209#3_0\n",
       "KASEMRAT_EB_0_1_XSXX                      459551209#3_1\n",
       "KASEMRAT_EB_0_2_XSXX                      459551209#3_2\n",
       "KASEMRAT_EB_0_3_XSRT                      459551209#3_3\n",
       "KASEMRAT_EB_1_0_LSXX                      459551209#0_0\n",
       "                                              ...      \n",
       "MASUKGRIDLOCK_ATTHAKAWI_SB_2_1_LXXX        27702347#6_1\n",
       "MASUKGRIDLOCK_ATTHAKAWI_SB_1_0_LSXT        27702347#4_0\n",
       "MASUKGRIDLOCK_ATTHAKAWI_SB_2_0_LXXX        27702347#6_0\n",
       "MASUKGRIDLOCK_SUKHUMVUT22_SB_0_0_XSXX    -453669106#1_0\n",
       "MASUKGRIDLOCK_SUKHUMVUT24_SB_0_0_XSXX     328942767#2_0\n",
       "Name: id, Length: 425, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME.loc[:,'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAMA4_WB_2_3_XSXX'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID.loc['820373198#0_3', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedetector_csv = pd.read_csv('namedetector_KASEMRAT_flow.csv')\n",
    "namedetector_df = pd.DataFrame(namedetector_csv, columns = ['name' , 'id'])\n",
    "NAME_D = namedetector_df.set_index('name')\n",
    "ID_D = namedetector_df.set_index('id')\n",
    "listdetector = open(\"namedetector_KASEMRAT_flow.txt\", \"r\")\n",
    "detector = {}\n",
    "for l in listdetector:\n",
    "    l = l.strip().split(' ')\n",
    "    if len(l)> 1:\n",
    "        d = []\n",
    "        for detec in l[2:]:\n",
    "            if type(NAME_D.loc[detec,'id']) == str:\n",
    "                d.append(NAME_D.loc[detec,'id'])\n",
    "            else : d.append(NAME_D.loc[detec,'id'][0])\n",
    "    if str(l[0])!= '':\n",
    "        detector[str(l[0])] = d\n",
    "list_detector= list(detector.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RAMA4_EB_1_4_XSXX</th>\n",
       "      <td>D459492917#0_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLONGTEI_EB_0_0_XSXX</th>\n",
       "      <td>D825786400_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLONGTEI_EB_0_1_XSXX</th>\n",
       "      <td>D825786400_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLONGTEI_EB_0_2_XSXX</th>\n",
       "      <td>D825786400_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLONGTEI_EB_0_3_XSXX</th>\n",
       "      <td>D825786400_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id\n",
       "name                                \n",
       "RAMA4_EB_1_4_XSXX     D459492917#0_4\n",
       "KLONGTEI_EB_0_0_XSXX    D825786400_0\n",
       "KLONGTEI_EB_0_1_XSXX    D825786400_1\n",
       "KLONGTEI_EB_0_2_XSXX    D825786400_2\n",
       "KLONGTEI_EB_0_3_XSXX    D825786400_3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME_D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KASEMRAT_EB_FPX_TP2', 50]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['KASEMRAT_EB_FPX_TP2',50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(NAME_D.loc['KLONGTEI_EB_0_0_XSXX','id'])\n",
    "# for i in range(len(list(detector.keys()))):\n",
    "#     print(i , list(detector.keys())[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot_encoding_current_phase():\n",
    "    number_phase = [4,9,7,5,4,3]\n",
    "    current_phase = [traci.trafficlight.getPhase(junction_id[key]) for key in junction_id.keys()]\n",
    "#     current_phase = [0,2,1,1,1,1]\n",
    "    hot_encoding_current_phase = np.array([])\n",
    "    for i in range(len(current_phase)):\n",
    "        binary_phase = np.zeros(number_phase[i])\n",
    "        binary_phase[current_phase[i]] = 1\n",
    "        hot_encoding_current_phase = np.concatenate((hot_encoding_current_phase, binary_phase), axis=None)\n",
    "    return current_phase, hot_encoding_current_phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID.loc['']#, '820373198#0', '820373196#0', '482209831#0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_attention(current_phase, hot_encoding_current_phase):\n",
    "#     current_phase = [KLONGTEI phase, RAMA4 phase,NARANONG phase, SUNLAKAKHON phase, KASEMRAT phase, ATTHAKAWI_RAMA4 phase]\n",
    "    MAP_RAMA4 = {0: [['RAMA4_EB_FPX_TP1',92] , ['KASEMRAT_EB_FPX_TP2_RAMA4',80], ['NARANONG_SW_FPX_TP1',80]],\n",
    "                 1: [['RAMA4_EB_FP1_TP3',10], ['RAMA4_NB_FPX_TP5',12.77]],\n",
    "                 2: [['RAMA4_WB_FP2_TP4',10], ['RAMA4_EB_FPX_TP1',92], ['RAMA4_NB_FPX_TP5',12.77]],\n",
    "                3: [['RAMA4_EB_FP3_TP2',40], ['RAMA4_WB_FP3_TP2',8.8], ['RAMA4_EB_FPX_TP1',92], ['RAMA4_NB_FPX_TP5',12.77 ]],\n",
    "                4: [['RAMA4_SB_FP4_TP5',10], ['RAMA4_EB_FPX_TP1',92], ['RAMA4_NB_FPX_TP5',12.77 ]],\n",
    "                5: [['RAMA4_NB_FP5_TP1',5], ['RAMA4_EB_FPX_TP1',92]],\n",
    "                6: [['KASEMRAT_EB_FPX_TP2_RAMA4',80]],\n",
    "                7: [['KASEMRAT_EB_FPX_TP2_RAMA4',80]],\n",
    "                8: [['KASEMRAT_EB_FPX_TP2_RAMA4',80]]}\n",
    "    MAP_KLONGTEI = {0: [['NARANONG_SB_FP5_TP6',80], ['RAMA4_EB_FP1_TP3',10]],\n",
    "                    1: [['RAMA4_EB_FP3_TP2',40], ['RAMA4_EB_FP1_TP3',10]],\n",
    "                    2: [['KLONGTEI_NB_FP3_TPX',10]],\n",
    "                   3: [['RAMA4_EB_FP3_TP2',40], ['RAMA4_EB_FP1_TP3',10]] }\n",
    "    MAP_NARANONG = {0: [['RAMA4_NB_FPX_TP5',12.77]],\n",
    "                1: [['NARANONG_SB_FP1_TP2',30], ['NARANONG_EB_FP1_TP2',10], ['NARANONG_WB_FPX_TP2',83.17]],\n",
    "                2: [['NARANONG_WB_FP2_TP3',5], ['NARANONG_WB_FP2_TP3_FLOW',70.83], ['NARANONG_SW_FPX_TP1',70.8], \n",
    "                   ['NARANONG_WB_FPX_TP2',83.17]],\n",
    "                3: [['NARANONG_EB_FP3_TP4',10], ['NARANONG_SW_FPX_TP1',70.83], ['NARANONG_WB_FPX_TP2',83.17]],\n",
    "                4: [['NARANONG_WB_FP4_TP5',5], ['NARANONG_SW_FPX_TP1',70.83], ['NARANONG_WB_FPX_TP2',83.17]],\n",
    "                5: [['NARANONG_SB_FP5_TP6',5], ['NARANONG_SW_FPX_TP1',70.83],['NARANONG_WB_FPX_TP2',83.17]], \n",
    "                6: [['NARANONG_NB_FP6_TP1',5], ['NARANONG_SW_FPX_TP1',70.83], ['NARANONG_WB_FPX_TP2',83.17]] }\n",
    "    MAP_SUNLAKAKHON = {0: [['NARANONG_WB_FPX_TP2',83.17]],\n",
    "                    1: [['SUNLAKAKHON_SB_FP1_TPX',10]],\n",
    "                    2: [['SUNLAKAKHON_NB_FP2_TP3',10], ['SUNLAKAKHON_SB_FPX_TP1',72.5]],\n",
    "                    3:[['SUNLAKAKHON_EB_FP3_TP4',10], ['SUNLAKAKHON_WB_FP3_TP4',55.09], ['SUNLAKAKHON_SB_FPX_TP1',72.5]],\n",
    "                    4:[['SUNLAKAKHON_SB_FP4_TP1',10], ['SUNLAKAKHON_SB_FPX_TP1',72.5]] }\n",
    "    MAP_KASEMRAT = {0: [['SUNLAKAKHON_SB_FPX_TP1',80], ['MASUKGRIDLOCK_ARI_NB_FPX_TP2',20]],\n",
    "                    1:[['KASEMRAT_EB_FPX_TP2',35.83], ['KASEMRAT_NB_FPX_TP3',90], ['MASUKGRIDLOCK_ARI_NB_FPX_TP2',20],\\\n",
    "                       ['MASUKGRIDLOCK_SUKHUMVUT_FPX_TP2',50]],\n",
    "                    2:[['KASEMRAT_EB_FP2_TP1',20], ['KASEMRAT_NB_FPX_TP3',90]],\n",
    "                    3:[['KASEMRAT_EB_FPX_TP2',35.83], ['MASUKGRIDLOCK_ARI_NB_FPX_TP2',20], ['MASUKGRIDLOCK_SUKHUMVUT_FPX_TP2',50]]}\n",
    "    MAP_ATTHAKAWI_RAMA4 = {0:[['KASEMRAT_EB_FPX_TP2',50]],\n",
    "                           1: [['MASUKGRIDLOCK_SUKHUMVUT_FPX_TP2',50], ['MASUKGRIDLOCK_ARI_NB_FPX_TP2', 20]],\n",
    "                          2:[['KASEMRAT_EB_FPX_TP2_RAMA4',85]]}\n",
    "    MAP = [MAP_KLONGTEI, MAP_RAMA4, MAP_NARANONG, MAP_SUNLAKAKHON,  MAP_KASEMRAT, MAP_ATTHAKAWI_RAMA4]\n",
    "    state_attention = np.zeros(31)\n",
    "    for i in range(len(current_phase)):\n",
    "        for e in MAP[i][current_phase[i]]:\n",
    "            occupancy = get_occupancy_average_percent(detector[e[0]]) \n",
    "            Index_detector = list_detector.index(e[0]) \n",
    "            if occupancy <= e[1]:\n",
    "                state_attention[Index_detector] = 1\n",
    "#     print(state_attention)\n",
    "    state = np.concatenate((state_attention, hot_encoding_current_phase), axis=None)\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the environment\n",
    "def start():\n",
    "    sumoBinary = checkBinary('sumo')\n",
    "    traci.start([sumoBinary, \"-c\", \"KASEMRAT-SUMO-UsingBookNetFile/osm.sumocfg\",\n",
    "                             \"--tripinfo-output\", \"tripinfo.xml\", '--start','true','--quit-on-end','true','--time-to-teleport','-1',\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occupancy_average_percent(detector_id): \n",
    "    #get occupancy average for all detector in list of detector_id and scale by (Vehicle Length + MinimumGap)/MinimumGap \n",
    "    #Vehicle Length = 4.62 MinimumGap = 2.37\n",
    "    occupancy = (sum([traci.lanearea.getLastStepOccupancy(e) for e in detector_id])/len(detector_id))*((4.62+2.37)/4.62)\n",
    "    return occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_throughput():\n",
    "    loopID = traci.inductionloop.getIDList()\n",
    "    throughput = sum([traci.inductionloop.getLastStepVehicleNumber(i) for i in loopID if traci.inductionloop.getLastStepMeanSpeed(i) > 0])\n",
    "    return throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backlog():\n",
    "#     laneID = traci.lane.getIDList()\n",
    "    backlog = sum([traci.lanearea.getLastStepVehicleNumber(i) for i in NAME.loc[:,'id']])\n",
    "    return backlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward():\n",
    "    throughput = 0\n",
    "    for i in range(5):\n",
    "        traci.simulationStep()\n",
    "        throughput += get_throughput()\n",
    "    \n",
    "    backlog = get_backlog()\n",
    "    reward = throughput - 0.004*backlog\n",
    "    return reward, throughput, backlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_phase = [4,9,7,5,4,3]\n",
    "def set_current_phase(action, current_phase):\n",
    "    if action < 4:\n",
    "        phase = action\n",
    "        current_phase[0] = phase\n",
    "    elif action < 13:\n",
    "        phase = (action-4)\n",
    "        current_phase[1] = phase\n",
    "    elif action < 20:\n",
    "        phase = (action-13)\n",
    "        current_phase[2] = phase\n",
    "    elif action < 25:\n",
    "        phase = (action-20)\n",
    "        current_phase[3] = phase\n",
    "    elif action < 29:\n",
    "        phase = (action-25)\n",
    "        current_phase[4] = phase\n",
    "    else:\n",
    "        phase = (action-29)\n",
    "        current_phase[5] = phase\n",
    "    for i in range (6):\n",
    "        traci.trafficlight.setPhase(junction_id[junction_name[i]], current_phase[i])\n",
    "    if current_phase[5] == 2:\n",
    "        traci.lane.setAllowed(NAME.loc['MASUKGRIDLOCK_ATTHAKAWI_SB_1_0_LSXT', 'id'], ['passenger'])\n",
    "        traci.lane.setAllowed(NAME.loc['MASUKGRIDLOCK_ATTHAKAWI_SB_2_0_LXXX', 'id'], ['passenger'])\n",
    "        traci.lane.setAllowed(NAME.loc['MASUKGRIDLOCK_ATTHAKAWI_SB_0_0_XSRT', 'id'], ['passenger'])\n",
    "    elif current_phase[5] == 1:\n",
    "        traci.lane.setDisallowed(NAME.loc['MASUKGRIDLOCK_ATTHAKAWI_SB_1_0_LSXT', 'id'], ['passenger'])\n",
    "        traci.lane.setDisallowed(NAME.loc['MASUKGRIDLOCK_ATTHAKAWI_SB_2_0_LXXX', 'id'], ['passenger'])\n",
    "        traci.lane.setDisallowed(NAME.loc['MASUKGRIDLOCK_ATTHAKAWI_SB_0_0_XSRT', 'id'], ['passenger'])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_memory = []\n",
    "def plot_durations():\n",
    "    print('show')\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(reward_memory, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv3(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.count = 0\n",
    "        self.reward = 0\n",
    "        self.rewards = 0\n",
    "        self.throughputs = 0\n",
    "        self.backlogs = 0\n",
    "        self.current_phase = [1,1,1,1,1,1]\n",
    "        self.done = False\n",
    "        self.reward_memory = []\n",
    "        self.action_space = gym.spaces.Discrete(31)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(62,), dtype=np.int8)\n",
    "        dateTimeObj = datetime.datetime.now()\n",
    "        self.dateTimeObj = dateTimeObj.strftime(\"%d-%b-%Y-%H-%M-%S\")\n",
    "        print(self.dateTimeObj)\n",
    "        with open( \"./Raytest/ray_results/\"+self.dateTimeObj+\".csv\" , 'a', newline='') as csv_file:\n",
    "                header = ['rewards', 'throughput','backlog']\n",
    "                writer = csv.DictWriter(csv_file, fieldnames = header)\n",
    "                writer.writeheader()\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.reward_memory.append(self.rewards)\n",
    "        self.count = 0\n",
    "        start()\n",
    "        self.reward = 0\n",
    "        self.rewards = 0\n",
    "        self.throughputs = 0\n",
    "        self.backlogs = 0\n",
    "        print(self.reward_memory)\n",
    "        current_phase, hot_encoding_current_phase = get_hot_encoding_current_phase()\n",
    "        state = get_state_attention(current_phase, hot_encoding_current_phase)\n",
    "#         print(state)\n",
    "        self.done = False\n",
    "        self.current_phase = current_phase\n",
    "        if len(self.reward_memory)%100 == 0:\n",
    "            print('memory',self.reward_memory[-10:])\n",
    "        return state \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        set_current_phase(action, self.current_phase)\n",
    "#         print(action)\n",
    "        current_phase, hot_encoding_current_phase = get_hot_encoding_current_phase()\n",
    "        self.reward, throughput, backlog= get_reward()\n",
    "        self.rewards += self.reward\n",
    "        self.throughputs += throughput\n",
    "        self.backlogs += backlog\n",
    "        state = get_state_attention(current_phase, hot_encoding_current_phase)\n",
    "        if np.isnan(self.reward) == True:\n",
    "            print('HELP', type(self.reward))\n",
    "        self.count += 1\n",
    "        self.current_phase = current_phase\n",
    "#         print('count', self.count)\n",
    "        self.done = False\n",
    "        if self.count >= 2880: #2880\n",
    "            traci.close()\n",
    "            self.done = True\n",
    "            with open( \"Raytest/ray_results/\"+self.dateTimeObj+\".csv\" , 'a', newline='') as csv_file:\n",
    "                header = ['rewards', 'throughput','backlog']\n",
    "                writer = csv.DictWriter(csv_file, fieldnames = header)\n",
    "                writer.writerow({'rewards': self.rewards, \n",
    "                                 'throughput': self.throughputs,\n",
    "                                'backlog': self.backlogs})\n",
    "\n",
    "        \n",
    "        return_state = np.array(state).astype(np.int8)\n",
    "#         print(return_state)\n",
    "        info = {\"throughput\": throughput,\n",
    "                \"backlog\":backlog\n",
    "                }\n",
    "        info = {**info}\n",
    "        print(info)\n",
    "        return return_state , self.reward, self.done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n",
      "06-Apr-2021-21-32-11\n",
      "[0]\n",
      "{'throughput': 0, 'backlog': 4}\n",
      "{'throughput': 0, 'backlog': 7}\n",
      "{'throughput': 0, 'backlog': 11}\n",
      "{'throughput': 1, 'backlog': 16}\n",
      "{'throughput': 1, 'backlog': 21}\n",
      "{'throughput': 0, 'backlog': 30}\n",
      "{'throughput': 0, 'backlog': 30}\n",
      "{'throughput': 1, 'backlog': 41}\n",
      "{'throughput': 0, 'backlog': 45}\n",
      "{'throughput': 1, 'backlog': 52}\n",
      "{'throughput': 2, 'backlog': 65}\n",
      "{'throughput': 0, 'backlog': 71}\n",
      "{'throughput': 2, 'backlog': 81}\n",
      "{'throughput': 1, 'backlog': 88}\n",
      "{'throughput': 9, 'backlog': 93}\n",
      "{'throughput': 8, 'backlog': 100}\n",
      "{'throughput': 5, 'backlog': 112}\n",
      "{'throughput': 7, 'backlog': 116}\n",
      "{'throughput': 3, 'backlog': 126}\n",
      "{'throughput': 3, 'backlog': 145}\n",
      "{'throughput': 4, 'backlog': 142}\n",
      "{'throughput': 8, 'backlog': 150}\n",
      "{'throughput': 7, 'backlog': 165}\n",
      "{'throughput': 2, 'backlog': 163}\n",
      "{'throughput': 5, 'backlog': 181}\n",
      "{'throughput': 8, 'backlog': 184}\n",
      "{'throughput': 5, 'backlog': 185}\n",
      "{'throughput': 0, 'backlog': 194}\n",
      "{'throughput': 3, 'backlog': 204}\n",
      "{'throughput': 6, 'backlog': 213}\n",
      "{'throughput': 8, 'backlog': 220}\n",
      "{'throughput': 5, 'backlog': 233}\n",
      "{'throughput': 1, 'backlog': 253}\n",
      "{'throughput': 2, 'backlog': 260}\n",
      "{'throughput': 6, 'backlog': 273}\n",
      "{'throughput': 4, 'backlog': 280}\n",
      "{'throughput': 5, 'backlog': 284}\n",
      "{'throughput': 7, 'backlog': 288}\n",
      "{'throughput': 12, 'backlog': 293}\n",
      "{'throughput': 5, 'backlog': 303}\n",
      "{'throughput': 7, 'backlog': 318}\n",
      "{'throughput': 9, 'backlog': 327}\n",
      "{'throughput': 5, 'backlog': 339}\n",
      "{'throughput': 10, 'backlog': 345}\n",
      "{'throughput': 16, 'backlog': 346}\n",
      "{'throughput': 13, 'backlog': 348}\n",
      "{'throughput': 27, 'backlog': 349}\n",
      "{'throughput': 24, 'backlog': 355}\n",
      "{'throughput': 23, 'backlog': 346}\n",
      "{'throughput': 21, 'backlog': 365}\n",
      "{'throughput': 16, 'backlog': 359}\n",
      "{'throughput': 9, 'backlog': 363}\n",
      "{'throughput': 8, 'backlog': 377}\n",
      "{'throughput': 5, 'backlog': 379}\n",
      "{'throughput': 7, 'backlog': 401}\n",
      "{'throughput': 12, 'backlog': 394}\n",
      "{'throughput': 9, 'backlog': 401}\n",
      "{'throughput': 10, 'backlog': 409}\n",
      "{'throughput': 13, 'backlog': 415}\n",
      "{'throughput': 14, 'backlog': 426}\n",
      "{'throughput': 8, 'backlog': 446}\n",
      "{'throughput': 8, 'backlog': 448}\n",
      "{'throughput': 8, 'backlog': 458}\n",
      "{'throughput': 4, 'backlog': 463}\n",
      "{'throughput': 8, 'backlog': 472}\n",
      "{'throughput': 7, 'backlog': 479}\n",
      "{'throughput': 7, 'backlog': 495}\n",
      "{'throughput': 10, 'backlog': 507}\n",
      "{'throughput': 6, 'backlog': 516}\n",
      "{'throughput': 5, 'backlog': 522}\n",
      "{'throughput': 5, 'backlog': 527}\n",
      "{'throughput': 5, 'backlog': 535}\n",
      "{'throughput': 8, 'backlog': 539}\n",
      "{'throughput': 11, 'backlog': 541}\n",
      "{'throughput': 7, 'backlog': 542}\n",
      "{'throughput': 3, 'backlog': 551}\n",
      "{'throughput': 10, 'backlog': 558}\n",
      "{'throughput': 12, 'backlog': 566}\n",
      "{'throughput': 10, 'backlog': 576}\n",
      "{'throughput': 12, 'backlog': 579}\n",
      "{'throughput': 14, 'backlog': 577}\n",
      "{'throughput': 15, 'backlog': 584}\n",
      "{'throughput': 26, 'backlog': 579}\n",
      "{'throughput': 20, 'backlog': 580}\n",
      "{'throughput': 13, 'backlog': 594}\n",
      "{'throughput': 13, 'backlog': 602}\n",
      "{'throughput': 11, 'backlog': 622}\n",
      "{'throughput': 15, 'backlog': 611}\n",
      "{'throughput': 11, 'backlog': 607}\n",
      "{'throughput': 9, 'backlog': 614}\n",
      "{'throughput': 3, 'backlog': 624}\n",
      "{'throughput': 2, 'backlog': 627}\n",
      "{'throughput': 4, 'backlog': 637}\n",
      "{'throughput': 10, 'backlog': 639}\n",
      "{'throughput': 6, 'backlog': 642}\n",
      "{'throughput': 14, 'backlog': 640}\n",
      "{'throughput': 11, 'backlog': 651}\n",
      "{'throughput': 14, 'backlog': 660}\n",
      "{'throughput': 16, 'backlog': 669}\n",
      "{'throughput': 17, 'backlog': 671}\n",
      "{'throughput': 12, 'backlog': 681}\n",
      "{'throughput': 18, 'backlog': 688}\n",
      "{'throughput': 21, 'backlog': 690}\n",
      "{'throughput': 14, 'backlog': 701}\n",
      "{'throughput': 12, 'backlog': 704}\n",
      "{'throughput': 12, 'backlog': 722}\n",
      "{'throughput': 10, 'backlog': 726}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"asd\")\n",
    "    env = MyEnv3(env_config=None)\n",
    "\n",
    "    for i_episode in range(1):\n",
    "        observation = env.reset()\n",
    "        for t in range(3000):\n",
    "            # env.render()\n",
    "#             print(observation)\n",
    "            action = env.action_space.sample()\n",
    "            # print('action' ,action)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\")\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# import ray.tune  as tune\n",
    "# from ray.tune import Callback\n",
    "# from ray.rllib.utils import merge_dicts\n",
    "# from ray.rllib.agents.dqn.dqn import calculate_rr_weights, \\\n",
    "#     DEFAULT_CONFIG as DQN_CONFIG, DQNTrainer, validate_config\n",
    "# # env = gym.make('CartPole-v0')\n",
    "\n",
    "\n",
    "# class MyCallback(Callback):\n",
    "#     def on_trial_start(self, iteration, trials, trial, **info):\n",
    "#         print(f\"I am in callback. This is iteration {iteration} inside trial {trial}\")\n",
    "# #         dateTimeObj = datetime.datetime.now()\n",
    "# #         dateTimeObj = dateTimeObj.strftime(\"%d-%b-%Y-%H-%M-%S-%f\")\n",
    "# #         with open( \"./Raytest/ray_results/\"+dateTimeObj+\".csv\" , 'a', newline='') as csv_file:\n",
    "# #                 header = ['rewards', 'throughput','backlog']\n",
    "# #                 writer = csv.DictWriter(csv_file, fieldnames = header)\n",
    "# #                 writer.writeheader()\n",
    "# #         print(info)\n",
    "#     def on_trial_result(self, iteration, trials, trial, result, **info):\n",
    "#         print(\"I am in second callback. Got result:\", info)\n",
    "# #         with open( \"./Raytest/ray_results/\"+str(trial)+\".csv\" , 'a', newline='') as csv_file:\n",
    "# #             header = ['rewards']\n",
    "# #             writer = csv.DictWriter(csv_file, fieldnames = header)\n",
    "# #             writer.writeheader()\n",
    "# #             writer.writerow({'rewards': result[\"episode_reward_mean\"]})\n",
    "            \n",
    "\n",
    "# def trial_name_id(trial):\n",
    "#     return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ray.shutdown()\n",
    "#     ray.init(\n",
    "# #              object_store_memory=int(1e9),  # 4gb\n",
    "# #              redis_max_memory=int(1e9)  #2gb\n",
    "#              )\n",
    "#     experiment_spec = tune.Experiment(\n",
    "#         trial_name_creator=trial_name_id,\n",
    "#         name = \"experiment_apex6\",\n",
    "#         run = \"APEX\",\n",
    "#         local_dir = \"./Raytest/ray_results\",\n",
    "#         checkpoint_freq = 3,\n",
    "#         checkpoint_at_end = True,\n",
    "#         log_to_file=True,\n",
    "#         config = {\n",
    "# #         \"optimizer\": merge_dicts(\n",
    "# #             DQN_CONFIG[\"optimizer\"], {\n",
    "# #                 \"max_weight_sync_delay\": 400,\n",
    "# #                 \"num_replay_buffer_shards\": 4,\n",
    "# #                 \"debug\": False\n",
    "# #             }),\n",
    "# #         \"n_step\": 3,\n",
    "        \n",
    "#             \"adam_epsilon\": 1e-8,\n",
    "#             # If not None, clip gradients during optimization at this value\n",
    "            \n",
    "#         \"num_gpus\": 0,\n",
    "#         \"dueling\": False,\n",
    "#         \"double_q\": False,\n",
    "#         \"num_workers\": 15,\n",
    "#         \"buffer_size\": 100000,\n",
    "#         \"framework\": \"tf\",\n",
    "#         \"learning_starts\": 28800, #2160\n",
    "#         \"train_batch_size\": 540,\n",
    "# #             \"num_samples\": 20,\n",
    "#         \"rollout_fragment_length\": 50,\n",
    "#         \"target_network_update_freq\": 540,\n",
    "#         \"prioritized_replay\": True,\n",
    "#         \"timesteps_per_iteration\": 2880, #2880\n",
    "# #         \"exploration_config\": {\"type\": \"PerWorkerEpsilonGreedy\"},\n",
    "# #         \"worker_side_prioritization\": True,\n",
    "# #         \"min_iter_time_s\": 30,\n",
    "#         # If set, this will fix the ratio of replayed from a buffer and learned\n",
    "#         # on timesteps to sampled from an environment and stored in the replay\n",
    "#         # buffer timesteps. Otherwise, replay will proceed as fast as possible.\n",
    "# #         \"training_intensity\": None,\n",
    "#          \"worker_side_prioritization\": False,\n",
    "#          \"lr\": 5e-4,\n",
    "#             \"gamma\": 0.6,\n",
    "\n",
    "            \n",
    "#             \"env\": MyEnv3 ,\n",
    "\n",
    "            \n",
    "#             }\n",
    "        \n",
    "#         )\n",
    "#     results = tune.run_experiments(experiment_spec, \n",
    "#                                   callbacks=[MyCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward_memory = []\n",
    "# def plot_durations():\n",
    "#     print('show')\n",
    "#     plt.figure(2)\n",
    "#     plt.clf()\n",
    "#     durations_t = torch.tensor(reward_memory, dtype=torch.float)\n",
    "#     plt.title('Training...')\n",
    "#     plt.xlabel('Step')\n",
    "#     plt.ylabel('Reward')\n",
    "#     plt.plot(durations_t.numpy())\n",
    "#     # Take 100 episode averages and plot them too\n",
    "#     if len(durations_t) >= 100:\n",
    "#         means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "#         means = torch.cat((torch.zeros(99), means))\n",
    "#         plt.plot(means.numpy())\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = MyEnv3(env_config=None)\n",
    "# action_space_size = env.action_space.n\n",
    "# state_space_size = 2^57\n",
    "# columns_name = ['state']+list(range(0, 30))\n",
    "# q_table = pd.DataFrame(columns = columns_name).set_index('state')\n",
    "\n",
    "# print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_episodes = 1000\n",
    "# max_steps_per_episode = 2880 # but it won't go higher than 1\n",
    "\n",
    "# learning_rate = 0.1\n",
    "# discount_rate = 0.99\n",
    "\n",
    "# exploration_rate = 1\n",
    "# max_exploration_rate = 1\n",
    "# min_exploration_rate = 0.01\n",
    "\n",
    "# exploration_decay_rate = 0.01 #if we decrease it, will learn slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards_all_episodes = []\n",
    "\n",
    "# # Q-Learning algorithm\n",
    "# for episode in range(num_episodes):\n",
    "#     state = env.reset()\n",
    "#     state=''.join(list(str(s) for s in state))\n",
    "#     print(state)\n",
    "#     done = False\n",
    "#     rewards_current_episode = 0\n",
    "    \n",
    "#     for step in range(max_steps_per_episode):\n",
    "#         if state not in q_table.index:\n",
    "#             q_table.loc[state,:] = 0\n",
    "#         # Exploration -exploitation trade-off\n",
    "#         exploration_rate_threshold = random.uniform(0,1)\n",
    "#         if exploration_rate_threshold > exploration_rate: \n",
    "#             action = (q_table[state,:].idxmax())\n",
    "#         else:\n",
    "#             action = env.action_space.sample()\n",
    "            \n",
    "#         new_state, reward, done, info = env.step(action)\n",
    "#         new_state=''.join(list(str(s) for s in new_state))\n",
    "#         # Update Q-table for Q(s,a)\n",
    "#         if new_state not in q_table.index:\n",
    "#             q_table.loc[new_state,:] = 0\n",
    "#         q_table.loc[str(state), action] = (1 - learning_rate) * q_table.loc[str(state), action] + \\\n",
    "#             learning_rate * (reward + discount_rate * (q_table.loc[new_state,:].max()))\n",
    "            \n",
    "#         state = new_state\n",
    "#         rewards_current_episode += reward\n",
    "        \n",
    "#         if done == True: \n",
    "#             break\n",
    "            \n",
    "#     # Exploration rate decay\n",
    "#     exploration_rate = min_exploration_rate + \\\n",
    "#         (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    \n",
    "#     rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "#     if episode%1 == 0:\n",
    "#         plt.figure(2)\n",
    "#         plt.clf() \n",
    "#         line_up = plt.plot(rewards_all_episodes)\n",
    "#         plt.legend([line_up], ['reward'])\n",
    "#         plt.pause(0.001)\n",
    "    \n",
    "# # Calculate and print the average reward per 10 episodes\n",
    "# rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / 100)\n",
    "# count = 100\n",
    "# print(\"********** Average  reward per thousand episodes **********\\n\")\n",
    "\n",
    "# for r in rewards_per_thousand_episodes:\n",
    "#     print(count, \": \", str(sum(r / 100)))\n",
    "#     count += 100\n",
    "    \n",
    "# # Print updated Q-table\n",
    "# print(\"\\n\\n********** Q-table **********\\n\")\n",
    "# print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     print(\"asd\")\n",
    "#     env = MyEnv3(env_config=None)\n",
    "\n",
    "#     for i_episode in range(1):\n",
    "#         observation = env.reset()\n",
    "#         for t in range(1000):\n",
    "#             # env.render()\n",
    "# #             print(observation)\n",
    "#             action = env.action_space.sample()\n",
    "# #             print('action' ,action)\n",
    "#             observation, reward, done, info = env.step(action)\n",
    "#             if done:\n",
    "#                 print(\"Episode finished after {} timesteps\")\n",
    "#                 break\n",
    "#     env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# import ray.tune  as tune\n",
    "# from ray.tune import Callback\n",
    "# from ray.rllib.utils import merge_dicts\n",
    "# from ray.rllib.agents.dqn.dqn import calculate_rr_weights, \\\n",
    "#     DEFAULT_CONFIG as DQN_CONFIG, DQNTrainer, validate_config\n",
    "# # env = gym.make('CartPole-v0')\n",
    "\n",
    "\n",
    "# class MyCallback(Callback):\n",
    "#     def on_trial_start(self, iteration, trials, trial, **info):\n",
    "#         print(f\"I am in callback. This is iteration {iteration} inside trial {trial}\")\n",
    "#         print(info)\n",
    "#     def on_trial_result(self, iteration, trials, trial, result, **info):\n",
    "#         print(\"I am in second callback. Got result:\", info, result)\n",
    "#         with open( \"/Raytest/ray_results/\"+str(trial)+\".csv\" , 'a', newline='') as csv_file:\n",
    "#             header = ['rewards']\n",
    "#             writer = csv.DictWriter(csv_file, fieldnames = header)\n",
    "#             writer.writeheader()\n",
    "#             writer.writerow({'rewards': result[\"episode_reward_mean\"]})\n",
    "            \n",
    "\n",
    "# def trial_name_id(trial):\n",
    "#     return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ray.shutdown()\n",
    "#     ray.init(\n",
    "# #              object_store_memory=int(1e9),  # 4gb\n",
    "# #              redis_max_memory=int(1e9)  #2gb\n",
    "#              )\n",
    "#     experiment_spec = tune.Experiment(\n",
    "#         trial_name_creator=trial_name_id,\n",
    "#         name = \"experiment_apex5\",\n",
    "#         run = \"APEX\",\n",
    "#         local_dir = \"/Raytest/ray_results\",\n",
    "#         checkpoint_freq = 3,\n",
    "#         checkpoint_at_end = True,\n",
    "#         log_to_file=True,\n",
    "#         config = {\n",
    "# #         \"optimizer\": merge_dicts(\n",
    "# #             DQN_CONFIG[\"optimizer\"], {\n",
    "# #                 \"max_weight_sync_delay\": 400,\n",
    "# #                 \"num_replay_buffer_shards\": 4,\n",
    "# #                 \"debug\": False\n",
    "# #             }),\n",
    "# #         \"n_step\": 3,\n",
    "#         \"num_gpus\": 0,\n",
    "#         \"dueling\": False,\n",
    "#         \"double_q\": False,\n",
    "#         \"num_workers\": 4,\n",
    "#         \"buffer_size\": 100000,\n",
    "#         \"framework\": \"tf\",\n",
    "#         \"learning_starts\": 2160, #2160\n",
    "#         \"train_batch_size\": 540,\n",
    "# #             \"num_samples\": 20,\n",
    "#         \"rollout_fragment_length\": 50,\n",
    "#         \"target_network_update_freq\": 540,\n",
    "#         \"prioritized_replay\": True,\n",
    "#         \"timesteps_per_iteration\": 2880,\n",
    "# #         \"exploration_config\": {\"type\": \"PerWorkerEpsilonGreedy\"},\n",
    "# #         \"worker_side_prioritization\": True,\n",
    "# #         \"min_iter_time_s\": 30,\n",
    "#         # If set, this will fix the ratio of replayed from a buffer and learned\n",
    "#         # on timesteps to sampled from an environment and stored in the replay\n",
    "#         # buffer timesteps. Otherwise, replay will proceed as fast as possible.\n",
    "# #         \"training_intensity\": None,\n",
    "#          \"worker_side_prioritization\": False,\n",
    " \n",
    "#             \"gamma\": 0.7,\n",
    "\n",
    "            \n",
    "#             \"env\": MyEnv3 ,\n",
    "\n",
    "            \n",
    "#             }\n",
    "        \n",
    "#         )\n",
    "#     results = tune.run_experiments(experiment_spec, \n",
    "#                                   callbacks=[MyCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       1., 0., 0., 1., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
